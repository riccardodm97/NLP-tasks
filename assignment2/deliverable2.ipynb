{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Fact checking, Neural Languange Inference (NLI)_**\n",
    "\n",
    "**Authors**: Giacomo Berselli, Marco Cucè, Riccardo De Matteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries and modules \n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "import string \n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from collections import OrderedDict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#setup logging \n",
    "log = logging.getLogger('logger')\n",
    "log.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('data/log.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we download the 'FEVER' dataset, unzip it and store the `.csv` document of each split in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_path = os.path.join(data_folder,'raw_dataset')   #path of the raw dataset as downloaded \n",
    "\n",
    "def save_response_content(response, destination):    \n",
    "    CHUNK_SIZE =32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks                \n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data():\n",
    "    zip_dataset_path = os.path.join(raw_dataset_path,'fever_data.zip')    \n",
    "    data_url_id =\"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"    \n",
    "    url =\"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(raw_dataset_path):        \n",
    "        os.makedirs(raw_dataset_path)\n",
    "\n",
    "    if not os.path.exists(zip_dataset_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:           \n",
    "            response = current_session.get(url, params={'id': data_url_id}, stream=True)\n",
    "\n",
    "        save_response_content(response, zip_dataset_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_dataset_path) as loaded_zip:            \n",
    "            loaded_zip.extractall(raw_dataset_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `.csv` files of the train, val and test splits, we encode all of them as a unique pandas Dataframe to be able to better inspect it and manipulate it as a whole.\n",
    "The Dataframe `df` is structured as follows: \n",
    "- `claim`: the fact to verify.\n",
    "- `evidence`: one of the possible multiple sentences in the dataset which supports or refutes the `claim`.\n",
    "- `id`: number associated to the fact to verify (different rows can have the same `id`).\n",
    "- `label`: whether the evidence REFUTES or SUPPORTS the claim.\n",
    "- `split`: the split to which one claim belongs (train, val or test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the entire dataset in a pandas dataframe and add the split column\n",
    "def encode_dataset(): \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for split in ['train','val','test']:\n",
    "        split_path = os.path.join(raw_dataset_path,f\"{split}_pairs.csv\")\n",
    "        split_df = pd.read_csv(split_path,index_col=0)\n",
    "        split_df['split'] = split\n",
    "\n",
    "        df = df.append(split_df,ignore_index=True,)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df \n",
    "\n",
    "df = encode_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the newly created dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "print('The splits present in the dataframe are:',df['split'].unique())\n",
    "print('Unique labels in the dataset:',df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can see that the dataset has been structured correctly.\\\n",
    "Now we print some values to check the dimensions of the different splits and to retrieve useful informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe shape:', df.shape)\n",
    "print('Number of example in train:',len(df[df['split']=='train']))\n",
    "print('Number of example in val:',len(df[df['split']=='val']))\n",
    "print('Number of example in test:',len(df[df['split']=='test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of claims in the training split of the dataset is clearly much higher than that of val and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should probably undergo some preprocessing before it can be used to train our model. Even if this was already noticeable from the few examples taken from the dataframe that we printed above, let's now show an example of an evidence to make more evident the work that we will need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.sample(1)['evidence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both claims and evidences contain a lot of unwanted text: punctuation, symbols, meta-characters, foreign words, tags, ecc. For some reason, claims are more clean than evidences. Nonetheless, we will preprocess both of them, to end up with a more manageable and digestible text. Especially since all the unwanted text do not contribute to the general meaning of each sentence, which is what we are interested in.\n",
    "Our preprocessing pipeline is composed of:\n",
    "- drop everything before the first `\\t` (every evidence seems to start with a number followed by `\\t`).\n",
    "- delete all unnecessary spaces (only one space between each word will be left). \n",
    "- remove all tabs and newlines characters (there are many `\\t` in the dataset).\n",
    "- remove the rounded parenthesis (`-LRB-` and `-RRB-`).\n",
    "- drop words inside square brackets (everything that falls between `-LSB-` and `-RSB-`).\n",
    "- drop everything after the last dot character (after that there are often some other words similar to tags which may be image descriptions or hyperlinks).\n",
    "- remove punctuation.\n",
    "- set everything to lowercase.\n",
    "\n",
    "Then we have defined three different types of preprocessing functions, which inherit from `preprocess_pipeline` and add new things, as follows:\n",
    "- Type 1: standard preprocessing which simply returns a list of words.\n",
    "- Type 2: transliterates UNICODE characters in ASCII, removes words with non ASCII characters, lemmatizes and returns a list of words.\n",
    "- Type 3: transliterates UNICODE characters in ASCII, removes words with non ASCII characters, applies stemming, remove stop-words and returns a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "def lemmatize_and_remove_non_ascii(sentence:str):\n",
    "    \"\"\"Remove unnecessary spaces, remove words with non ASCII characters and lemmatize\"\"\"\n",
    "    sentence = sentence.split() #remove all unnecessary spaces and return a list of words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence if word.isascii()] #if a word has all ASCII characters: lemmatize, else: remove\n",
    "    return sentence\n",
    "\n",
    "def stemm_and_remove_non_ascii(sentence: str):\n",
    "    sentence = sentence.split() #remove all unnecessary spaces and return a list of words\n",
    "    ps = PorterStemmer()\n",
    "    sentence = [ps.stem(word) for word in sentence if word.isascii()]#if a word has all ASCII characters: stemm, else: remove\n",
    "    return sentence\n",
    "\n",
    "def preprocess_pipeline(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing\"\"\"\n",
    "    \n",
    "    #drop everything before the first '\\t' \n",
    "    sentence = sentence[sentence.find('\\t')+1:]\n",
    "\n",
    "    #drop everything after the last period\n",
    "    period_idx = sentence.rfind('.')\n",
    "    if period_idx!= -1:\n",
    "        sentence = sentence[:period_idx]\n",
    "\n",
    "    #remove all rounded parenthesis \n",
    "    sentence = sentence.replace('-LRB-','').replace('-RRB-','')\n",
    "\n",
    "    #remove words inside square brackets\n",
    "    sentence = re.sub(\"-LSB.*?-RSB-\",\"\",sentence)\n",
    "\n",
    "    #remove all square brackets\n",
    "    sentence = sentence.replace('-LSB-','').replace('-RSB-','')\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = sentence.translate(str.maketrans(dict.fromkeys(string.punctuation,' ')))\n",
    "\n",
    "    #subsitute the character ˈ with a space \n",
    "    sentence = sentence.replace('ˈ',' ')\n",
    "\n",
    "    #put everything to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type1(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #remove all unnecessary spaces and return a list of words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type2(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing, transliterates UNICODE characters in ASCII, \n",
    "    remove words with non ASCII characters, lemmatize and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #transliterates any UNICODE string into the closest possible representation in ASCII text\n",
    "    sentence = unidecode.unidecode(sentence)\n",
    "\n",
    "    #remove non-ascii words\n",
    "    sentence = lemmatize_and_remove_non_ascii(sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type3(sentence: str):\n",
    "    \"\"\"\n",
    "        Apply standard preprocessing, removes stop-words and non ascii's,  and stemmes.\n",
    "    \"\"\"\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "    sentence = unidecode.unidecode(sentence)\n",
    "    stemmed = stemm_and_remove_non_ascii(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stop_words = [word for word in stemmed if not word in stop_words]\n",
    "    return filter_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our preprocessing pipeline we will apply it to an example in the dataset that we have identified to be a pretty tough one in terms of amount of cleanup necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve from the dataset the 13th example. It is one about Greece in which the text is pretty messy.\n",
    "\n",
    "original_claim = df.loc[13,'claim']\n",
    "original_evidence = df.loc[13,'evidence']\n",
    "\n",
    "processed_claim = preprocess_type2(original_claim)\n",
    "processed_evidence = preprocess_type2(original_evidence)\n",
    "\n",
    "print('Original claim:',original_claim)\n",
    "print('Processed claim:',processed_claim,'\\n')\n",
    "print('Original evidence:',original_evidence)\n",
    "print('Processed evidence:',processed_evidence,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the final results relative to both the claim and evidence after the preprocessing are satisfactory. For this reason we are now going to apply the preprocessing function to the entire dataset encoded as a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['claim'] = df['claim'].apply(preprocess_type2)\n",
    "df['evidence'] = df['evidence'].apply(preprocess_type2)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build the dictionaries that will be used for the numerical tokenization of the dataset and for the generation of the embedding matrix.\n",
    "\n",
    "The function `build_vocab` takes in input the list of unique words in the whole dataset and creates:\n",
    "- `word2int`: dictionary which associates each word with an integer.\n",
    "- `int2word`: dictionary which associates each integer with the relative word.\n",
    "\n",
    "These two dictionaries constitute a bijective mapping between words and indexes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "def build_vocab(unique_words : list[str]): \n",
    "    \"\"\"\n",
    "        Builds the dictionaries word2int, int2word and put them in the Vocabulary\n",
    "    \"\"\"\n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "    \n",
    "    return Vocab(word2int,int2word,unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `build_vocab` needs in input the list of all the unique words in the dataset, so we're now going to retrieve it from the dataset to be able to build the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_claim = df['claim'].explode().unique().tolist()  \n",
    "unique_words_evidence = df['evidence'].explode().unique().tolist()\n",
    "\n",
    "print('the number of unique words belonging to claims is:', len(unique_words_claim))\n",
    "print('the number of unique words belonging to evidences is:', len(unique_words_evidence))\n",
    "\n",
    "unique_words = set(unique_words_evidence + unique_words_claim)\n",
    "print('the number of unique words in the entire dataset is:', len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vocabulary which contains the mapping between word and index we can 'numberise' the dataset. In particular we will add to the Dataframe 3 columns:\n",
    "- `idx_claim`: same as `claim` but with each word substituted by its index.\n",
    "- `idx_evidence`: same as `evidence` but with each word substituted by its index.\n",
    "- `idx_label`: label encoded as a unique integer (0: REFUTES, 1: SUPPORTS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexed_dataframe(df: pd.DataFrame):\n",
    "\n",
    "    df['idx_claim'] = df.claim.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "    df['idx_evidence'] = df.evidence.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "\n",
    "    df['label'] = df.label.astype('category')   #convert the label column into category dtype\n",
    "    df['idx_label'] = df.label.cat.codes        #assign unique integer to each category\n",
    "\n",
    "    return df \n",
    "\n",
    "def check_dataframe_numberization(df,vocab):\n",
    "\n",
    "    \"\"\"\n",
    "       Checks if the numberized dataframe will lead to the normal dataframe usind the reverse mapping \n",
    "    \"\"\"\n",
    "\n",
    "    claims = df['claim']\n",
    "    evidences = df['evidence']\n",
    "\n",
    "    idx_to_claims = df.idx_claim.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "    idx_to_evidences = df.idx_evidence.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "\n",
    "    if claims.equals(idx_to_claims) and evidences.equals(idx_to_evidences):\n",
    "        print('All right with dataset numberization')\n",
    "    else:\n",
    "        raise Exception('There are problems with Dataset numberization')\n",
    "\n",
    "df = build_indexed_dataframe(df)\n",
    "\n",
    "check_dataframe_numberization(df,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the operation was successful, let's have a look at the numberized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Loaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate mini-batches for each split to be passed to the network we leveraged a `torchtext` utility, such as `BucketIterator`. It ensures that each mini-batch is composed of sequences of nearly the same length (depending on the chosen batch size), in order to add the minimum padding possible to each Tensor. In order to do so, we needed to create a Pytorch Dataset since this is what is requested by the Bucket Iterator.\\\n",
    "The problem now is how to define the length of the input to the model (which is used to create buckets of similar-lenghts sequences), since for this task we are dealing with multiple inputs (claim and evidence). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_len = df.claim.apply(len)\n",
    "evidence_len = df.evidence.apply(len)\n",
    "print('average length of a claim sentence:',claim_len.mean())\n",
    "print('average length of a evidence sentence:',evidence_len.mean())\n",
    "print('max difference in length of claim sentences:',claim_len.max() - claim_len.min())\n",
    "print('max difference in length of evidence sentences:',evidence_len.max() - evidence_len.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the fact that the average sentence length for an evidence is much bigger than for a claim, we decided to create buckets based on the length of the evidence and only with that being equal, based on the claim's length. So the mini-batches will be constructed by grouping similar-size evidences and their corresponding claims.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import BucketIterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "\n",
    "        dataframe = dataframe.copy()\n",
    "        self.claims = dataframe['idx_claim']      #column of numberized claims \n",
    "        self.evidences = dataframe['idx_evidence']   #column of numberized evidences \n",
    "        self.labels = dataframe['idx_label']       #column of categorical label \n",
    "        self.claim_ids = dataframe['id']          #column of claim ids \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'claim': self.claims[idx],\n",
    "                'evidence': self.evidences[idx],\n",
    "                'label': self.labels[idx],\n",
    "                'claim_id': self.claim_ids[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int, dataframe: pd.DataFrame):     #b_s = batch_size\n",
    "    \n",
    "    train_df = dataframe[dataframe['split'] == 'train'].reset_index(drop=True)      \n",
    "    val_df = dataframe[dataframe['split'] == 'val'].reset_index(drop=True)\n",
    "    test_df = dataframe[dataframe['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "    #create DataframeDataset objects for each split \n",
    "    train_dataset = DataframeDataset(train_df)\n",
    "    val_dataset = DataframeDataset(val_df)\n",
    "    test_dataset = DataframeDataset(test_df)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches and return an iterator for each split.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: (len(x['evidence']),len(x['claim'])), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that the dataloaders have been created correctly. In order to do that, we print the indexed claim, the indexed evidence and the claim id of the first element of the train dataloader's first batch, and then we look in the dataframe to see if the indexes correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_size = 128\n",
    "tr, vl, ts = create_dataloaders(temp_batch_size, df)\n",
    "random_idx = random.randint(0, temp_batch_size-1)\n",
    "tr.init_epoch()\n",
    "for batch_id, batch in enumerate(tr.batches):\n",
    "    print(\"Claim: \", batch[random_idx]['claim'])\n",
    "    print(\"Evidence: \", batch[random_idx]['evidence'])\n",
    "    print(\"Label: \", batch[random_idx]['label'])\n",
    "    print(\"Claim id: \", batch[random_idx]['claim_id'], \"\\n\")\n",
    "    print(\"Corresponding row in the dataset:\")\n",
    "    df[df['id'] == (batch[random_idx]['claim_id'])]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally build an embedding matrix that will be used by the embedding layer of our model to store pre-trained word embeddings and retrieve them using indices. \n",
    "The function `build_embedding_matrix`, via the passed embedding model and the `word2int` dictionary, costructs a matrix that stores at each word-index the corresponding embedding vector found in GloVe. In particular we decided to use Glove as embedding model with a vector dimension of 300. \n",
    " \n",
    "In order to handle OOV words:\n",
    "- If a word in the dataset (identified by its unique integer) is present in GloVe model, we store its embedding vector in the embedding matrix.\n",
    "- Otherwise we assign as embedding to the OOV word a random vector of size 300, sampled from a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we need to download the GloVe model from `gensim`.\\\n",
    "To avoid downloading the GloVe embeddings more than once, since the process is really slow, we store the `KeyedVectors` in the data folder. In case this is not the first run and it has been already created and saved we can load it from the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix_path = os.path.join(data_folder, \"emb_matrix.npy\")\n",
    "glove_model_path = os.path.join(data_folder, \"glove_vectors.txt\") \n",
    "\n",
    "def download_glove_emb(force_download = False):   \n",
    "    \"\"\"\n",
    "        Download the glove embedding model and returns it \n",
    "    \"\"\"\n",
    "    emb_model = None\n",
    "\n",
    "    if os.path.exists(glove_model_path) and not force_download: \n",
    "        print('glove vecotrs already saved in data folder, retrieving the file...')\n",
    "        emb_model = KeyedVectors.load_word2vec_format(glove_model_path, binary=True)\n",
    "        print('vectors loaded')\n",
    "\n",
    "    else:\n",
    "        print('downloading glove embeddings...')        \n",
    "        embedding_dimension=300\n",
    "\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        emb_model = gloader.load(download_path)\n",
    "\n",
    "        print('saving glove embeddings to file')  \n",
    "        emb_model.save_word2vec_format(glove_model_path, binary=True)\n",
    "        \n",
    "    return emb_model\n",
    "\n",
    "force_download = False      # to download glove model even if the vectors model has been already stored. Mainly for testing purposes\n",
    "\n",
    "glove_embeddings = download_glove_emb(force_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the glove embeddings, we can check if there are some Out Of Vocabulary (OOV) words in our processed dataset.\n",
    "\\\n",
    "A word is considered OOV if it is present in our dataset but not in the GloVe embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, vocab):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "    idx_oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "\n",
    "    else: \n",
    "        for word in vocab.unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "                idx_oov_words.append(vocab.word2int[word]) \n",
    "        \n",
    "        print(\"Total number of unique words in dataset:\",len(vocab.unique_words))\n",
    "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(vocab.unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,15))\n",
    "    \n",
    "    return oov_words, idx_oov_words\n",
    "\n",
    "oov_words, idx_oov_words = check_OOV_terms(glove_embeddings,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the GloVe embeddings for our embedding matrix and applying the 3 different types of preprocessing techniques developed, we obtain the following results:\n",
    "- Type 1: 3148 OOV words with 35096 as total number of unique words (8.97%).\n",
    "- Type 2: 2761 OOV words with 31692 as total number of unique words (8.71%).\n",
    "- Type 3: 8031 OOV words with 26952 as total number of unique words (29.80%).\n",
    "\n",
    "While the first two types seem to obtain similar outcomes, with the second type which not only reduces the OOV words but also the number of unique words of the dataset, the third type is probably too heavy. The main reason may be the stemming, because it truncates the words producing new ones which not always exist, with respect to the lemmatization.\n",
    "\n",
    "Now let's build the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors,vocab) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        If the embedding for the word is present, add it to the embedding_matrix, otherwise insert a vector of random values.\n",
    "        Return the embedding matrix\n",
    "    \"\"\"\n",
    "    if emb_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "        return None\n",
    "    \n",
    "    print('Building embedding matrix...')\n",
    "\n",
    "    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n",
    "    embedding_matrix = np.zeros((len(vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "    for word, idx in vocab.word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "    \n",
    "    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(glove_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first few rows of the freshly created embedding matrix, to get a sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embedding_matrix).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the very first row is full of zeros since that's a 'fake embedding' for the padding token which will never be used in practice.\n",
    "\n",
    "To be completely sure that the embedding matrix has been built correctly, we check that the embedding vector associated with an index in the embedding matrix is the same as the one retrieved from GloVe by passing to it the word to which that index correspond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_id_corr(glove: gensim.models.keyedvectors.KeyedVectors, vocab, matrix, dataframe):\n",
    "    \"\"\"\n",
    "        Checks whether the numberized dataframe and the index of the embedding matrix correspond\n",
    "    \"\"\"\n",
    "    if not glove:\n",
    "        print('WARNING: empty model, remember to download GloVe first or set force_dowload to True')\n",
    "        return \n",
    "    oov_words_ = []\n",
    "\n",
    "    for indexed_sentence in dataframe['idx_claim']+dataframe['idx_evidence']:\n",
    "\n",
    "        for token in indexed_sentence:\n",
    "            embedding = matrix[token]\n",
    "            word = vocab.int2word[token]\n",
    "            if word in glove.key_to_index:\n",
    "                assert(np.array_equal(embedding,glove[word]))\n",
    "            else:\n",
    "                oov_words_.append(word)\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))\n",
    "\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no error has been found, we can safely proceed with the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytoch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "#scikit-learn imports \n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use a dynamic approach, we define a single custom model which will build the correct architecture based on the paramaters passed to the tuple `Architecture`. In particular, the first parameter of the tuple stores the strategy chosen to embed the sentences of the claim and the evidence. You can choose between:\n",
    "- `mlp`: encode token sequences via a simple MLP layer.\n",
    "- `rnn_last`: encode token sequences via a RNN and take the last state as the sentence embedding.\n",
    "- `rnn_avg`: encode token sequences via a RNN and average all the output states.\n",
    "- `bag_of_vectors`: compute the sentence embedding as the mean of its token embeddings.\n",
    "\n",
    "The second parameter defines the technique chosen to merge evidence and claim sentence embeddings, as follows:\n",
    "- `concat`: define the classification input as the concatenation of evidence and claim sentence embeddings.\n",
    "- `sum`: define the classification input as the sum of evidence and claim sentence embeddings.\n",
    "- `mean`: define the classification input as the mean of evidence and claim sentence embeddings.\n",
    "\n",
    "The third parameter instead is a boolean and allows to add an additional feature, the cosine similarity. `cosine_sim` allows to see if some similarity information between the claim to verify and one of its associated evidence might be useful to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Architecture = namedtuple('Architecture',['sentence_emb_strat','merge_input','cosine_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our model architecture  \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix: np.ndarray, model_param : dict, architecture : Architecture, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.device = device\n",
    "        self.model_param = model_param\n",
    "        self.architecture = architecture\n",
    "\n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,model_param['pad_idx'],model_param['freeze_embedding'])\n",
    "\n",
    "        if self.architecture.sentence_emb_strat == 'mlp':\n",
    "            self.mlp = nn.Linear(model_param['max_tokens'],1)\n",
    "\n",
    "        elif self.architecture.sentence_emb_strat in ('rnn_last','rnn_avg'):\n",
    "            if self.model_param['rnn'] == 'lstm':\n",
    "                self.rnn = nn.LSTM(self.word_embedding_dim, self.word_embedding_dim, batch_first = True) \n",
    "            else:\n",
    "                self.rnn = nn.GRU(self.word_embedding_dim, self.word_embedding_dim, batch_first = True) \n",
    "\n",
    "        self.drop_layer = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #determine the input dimension of the last layer that will classify each claim \n",
    "        classifier_input_dim = int(self.architecture.cosine_sim) + (\n",
    "            self.word_embedding_dim * 2\n",
    "            if self.architecture.merge_input == \"concat\"\n",
    "            else self.word_embedding_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_dim,1)   \n",
    "\n",
    "\n",
    "        self.to(self.device)  #move model to device , 'gpu' if possible \n",
    "    \n",
    "    def get_name(self) -> str:\n",
    "\n",
    "        return self.architecture.sentence_emb_strat+'_'+self.architecture.merge_input+('_cos_sim' if self.architecture.cosine_sim else '' )\n",
    "\n",
    "    \n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze = True):\n",
    "    \n",
    "        matrix = torch.Tensor(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def pad_batch(self,batch: list):    #pad each sentece of a batch to a common length\n",
    "        \"\"\"\n",
    "            Input:  List of Tensors of variable length\n",
    "            Output: Batch of tensors all padded to the same length \n",
    "        \"\"\"\n",
    "        batch = batch.copy()\n",
    "        \n",
    "        #if we are going to use 'mlp' as sentence embedding strategy, all the sentences should be padded to max_tokens length\n",
    "        if self.architecture.sentence_emb_strat == 'mlp':\n",
    "            batch[0] = nn.ConstantPad1d((0,self.model_param['max_tokens']-batch[0].shape[0]),0)(batch[0])  \n",
    "\n",
    "        padded_batch = rnn.pad_sequence(batch,batch_first = True, padding_value = self.model_param['pad_idx'])\n",
    "\n",
    "        padded_batch = padded_batch.to(self.device)    #move tensor to gpu if possible \n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "    def words_embedding(self, word_idxs):   #get embedding vectors for each token in sentence \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens]\n",
    "            Output: [batch_size, num_tokens, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(word_idxs)\n",
    "    \n",
    "    def sentence_embedding(self, embeddings, sentence_lenghts):     #compute sentence embedding \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens, embedding_dim]\n",
    "            Output: [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        strat = self.architecture.sentence_emb_strat\n",
    "\n",
    "        def mlp():  #compute sentence embedding via dense layer \n",
    "            \n",
    "            reshaped_embeddings = embeddings.permute(0,2,1)     #swap last two dimensions since Linear operates only on last dimension\n",
    "\n",
    "            sentence_emb = self.mlp(reshaped_embeddings)   \n",
    "\n",
    "            return sentence_emb.squeeze(2)   #remove dimension of size 1 \n",
    "        \n",
    "        def rnn_last():    #take as sentence embedding the last state of the rnn \n",
    "            \n",
    "            packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_out, (last_h, _)  = self.rnn(packed_embeddings)   \n",
    "\n",
    "            if torch.isnan(last_h.squeeze(0)).any():    #debug \n",
    "                log.debug('nan in last_h')  \n",
    "                raise Exception(' nan in last h ')\n",
    "            \n",
    "            return last_h.squeeze(0)  #remove first dimension of 1 (TODO: if bidirectional or more than 1 layer this has to be handled)\n",
    "        \n",
    "        def rnn_avg():   #take as sentence embedding the average of all the states of the rnn corresponing to each word \n",
    "\n",
    "            packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_out, _  = self.rnn(packed_embeddings)\n",
    "\n",
    "            unpacked_out, l = pad_packed_sequence(packed_out,batch_first=True)\n",
    "\n",
    "            avg = unpacked_out.sum(dim=1).div(sentence_lenghts.unsqueeze(dim=1))\n",
    "\n",
    "            return avg\n",
    "        \n",
    "        def bag_of_vectors():  #sentence embedding as the mean of its token embeddings\n",
    "\n",
    "            avg = embeddings.sum(dim=1).div(embeddings.count_nonzero(dim=1))\n",
    "\n",
    "            return avg \n",
    "\n",
    "        if strat == 'mlp':\n",
    "            return mlp()\n",
    "        elif strat == 'rnn_last':\n",
    "            return rnn_last()\n",
    "        elif strat == 'rnn_avg':\n",
    "            return rnn_avg()\n",
    "        elif strat == 'bag_of_vectors':\n",
    "            return bag_of_vectors()\n",
    "        else :\n",
    "            raise Exception('Incorrect name for sentence embedding strategy')\n",
    "\n",
    "    def merge_sentence_emb(self,claims,evidences):\n",
    "        \"\"\"\n",
    "            Input:  claims -> [batch_size, embedding_dim] , evidences -> [batch_size, embedding_dim]\n",
    "            Output: [batch_size, dim] dim is based on strategy specified \n",
    "        \"\"\"\n",
    "\n",
    "        strat = self.architecture.merge_input\n",
    "\n",
    "        if strat == 'concat':\n",
    "            result = torch.cat((claims,evidences),dim=1)      #concatenate the two tensors \n",
    "        elif strat == 'sum': \n",
    "            result = torch.stack((claims,evidences), dim=0).sum(dim=0)    #sum the two tensors \n",
    "        elif strat == 'mean':\n",
    "            result = torch.stack((claims,evidences),dim=0).mean(dim=0)    #compute mean of the two tensors \n",
    "        else :\n",
    "            raise Exception('Incorrect name for input-merge strategy')\n",
    "        \n",
    "        if self.architecture.cosine_sim :\n",
    "            cosine_sim = F.cosine_similarity(claims,evidences).unsqueeze(-1)\n",
    "            result = torch.cat((result,cosine_sim),dim=1)                      #add to previously generated merged ouput, one value representing cosine similarity between the two input tensors \n",
    "\n",
    "        return result \n",
    "\n",
    "\n",
    "    def forward(self, claims, claim_lengths, evidences, evidence_lengths):\n",
    "\n",
    "        #pad the sentences to have fixed size \n",
    "        padded_claims = self.pad_batch(claims)\n",
    "        # print('padded_claims = ', padded_claims)\n",
    "        padded_evidences = self.pad_batch(evidences)\n",
    "        # print('padded_evidence = ', padded_evidences)\n",
    "        \n",
    "        #embed each word in a sentence with a 300d vector \n",
    "        word_emb_claims = self.words_embedding(padded_claims)       \n",
    "        # print('word_emb_claims = ', word_emb_claims)   \n",
    "        word_emb_evidences = self.words_embedding(padded_evidences)\n",
    "        # print('word_emb_evidences = ', word_emb_evidences)\n",
    "\n",
    "        #compute sentence embedding\n",
    "        sentence_emb_claims = self.sentence_embedding(word_emb_claims,claim_lengths)\n",
    "        # print('sentence_emb_claims = ', sentence_emb_claims)\n",
    "        sentence_emb_evidences = self.sentence_embedding(word_emb_evidences,evidence_lengths)\n",
    "        # print('sentence_emb_evidences = ', sentence_emb_evidences)\n",
    "\n",
    "        #merge multi-inputs \n",
    "        classification_input = self.merge_sentence_emb(sentence_emb_claims,sentence_emb_evidences)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.model_param['dropout']:\n",
    "            classification_input = self.drop_layer(classification_input)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(classification_input)\n",
    "\n",
    "        predictions = predictions.squeeze()   #remove dim of size 1 \n",
    "\n",
    "        return predictions \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we define the functions that will be used in the 'train & eval' and 'test' pipeline to compute the metrics of the models.\\\n",
    "In particular, there could be two types of performance evaluations:\n",
    "- Multi-input classification evaluation: this type of evaluation is the easiest and concerns computing evaluation metrics, such as accuracy and F1-score of the models on our pre-processed dataset. In other words, we assess the performance of chosen classifiers.\n",
    "- Claim verification evaluation: if we want to give an answer concerning the claim itself, we need to consider the whole evidence set. For a given claim, we consider all its corresponding (claim, evidence) pairs and their corresponding classification outputs. At this point, what we need to do is to compute the final predicted claim label via majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy and f1-score \n",
    "def acc_and_f1(y_true: torch.LongTensor,y_pred: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1\n",
    "\n",
    "#construct y_true and y_pred lists to be passed to acc_and_f1 function, but based on majority voting strategy\n",
    "def majority_voting(y_true, y_pred, y_ids):\n",
    "    \"\"\"\n",
    "        Input: the list of corresponing true labels, the list of predicted labels, the list of claim ids to compute majority voting\n",
    "        Output : the list of true labels (one for each claim id), the list of predicted labels via majority voting\n",
    "\n",
    "        Idea behind the implementation: Since there could be more claims with the same id in the dataset, we start by counting the\n",
    "        number of occurrences of each claim id in the dataset and we store them sorted on the id number (tensor 'a'). \n",
    "        Then we count for each id how many times we predict it as supported in the predicted tensor 'y_pred', where a 'SUPPORTS' \n",
    "        prediction is considered as a 1 (integer), so in practice we sum the 1s founded and again we store the results sorted on the \n",
    "        id number (tensor 'b'). Next we create a tensor 'true', containing the true label for each claim and sorted on the id number \n",
    "        (so if the true label of the first claim is supported, the integer at index 0 will be a 1), Finally we create the tensor \n",
    "        'pred' which verifies for each element of 'b' (so for each number of SUPPORTS for each id), if it is greater than the number \n",
    "        of occurrences of that id in the dataset divided by 2. This means that, if the result is positive, we have predicted that \n",
    "        claim in most cases as supported, otherwise as refuted (same number of SUPPORTS and REFUTES will be considered as REFUTES), and \n",
    "        we store it again in a tensor sorted on the id number.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    search_sorted = torch.searchsorted(y_ids.unique(),y_ids)\n",
    "    a = torch.bincount(search_sorted)           #number of occurrences for each id in the dataset\n",
    "    b = torch.bincount(search_sorted, y_pred)   #for each id how many 1s (SUPPORTS) there are in the predicted tensor\n",
    "    true = (torch.bincount(search_sorted, y_true) > 0).int()    #tensor (sorted on claim id) containing the true label for each claim (1: SUPPORTS, 0: REFUTES)\n",
    "    pred = (b > (a / 2)).int()  #tensor (sorted on claim id) containing the predicted label for each claim (1: SUPPORTS, 0: REFUTES)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    log.debug('maj voting: %s',end-start)\n",
    "\n",
    "    return true, pred\n",
    "\n",
    "#compute accuracy and f1 score via majority voting \n",
    "def acc_and_f1_majority(y_true, y_pred, y_ids):\n",
    "\n",
    "    y_true, y_pred = majority_voting(y_true,y_pred,y_ids)\n",
    "\n",
    "    acc, f1 = acc_and_f1(y_true,y_pred)\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train and Validation pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the train and validation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: Custom_model, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the model istantiated with pre-defined hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - optimizer: optimizer for backward pass \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.init_epoch()  #generate and shuffles batches from dataloader #TODO: create_batches \n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        claims_batch = [torch.LongTensor(example['claim']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "        evidences_batch = [torch.LongTensor(example['evidence']) for example in batch]     #list of tensors of tags id for each sentence in a batch \n",
    "\n",
    "        claims_lengths = torch.Tensor([len(example['claim']) for example in batch])         #lenght of each claim sentence before padding \n",
    "        evidence_lengths = torch.Tensor([len(example['evidence']) for example in batch])         #lenght of each evidence sentence before padding \n",
    "\n",
    "        target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "        target_ids = torch.LongTensor([example['claim_id'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "        #move tensors to gpu if possible \n",
    "        claims_lengths = claims_lengths.to(device)\n",
    "        evidence_lengths = evidence_lengths.to(device)\n",
    "        target_labels = target_labels.to(device)    \n",
    "\n",
    "        #zero the gradients \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad()            \n",
    "\n",
    "        predictions = model(claims_batch,claims_lengths,evidences_batch,evidence_lengths)   #generate predictions \n",
    "\n",
    "        loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "\n",
    "        pred = (predictions > 0.0 ).int().cpu()              #get class label \n",
    "\n",
    "        #concatenate the new tensors with the one computed in previous steps\n",
    "        all_pred = torch.cat((all_pred,pred))          \n",
    "        all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "        all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "        #backward pass \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()    #accumulate batch loss \n",
    "\n",
    "\n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    maj_acc, maj_f1 = acc_and_f1_majority(all_targ,all_pred,all_ids)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)    #mean loss \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('train epoch time: %s',end-start)\n",
    "\n",
    "    return loss, acc, f1, maj_acc, maj_f1,predictions\n",
    "\n",
    "\n",
    "def eval_loop(model: Custom_model, iterator: BucketIterator, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the sequence pos tagger model istantiated with fixed hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "     \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor() \n",
    "    \n",
    "    model.eval()   #model in eval mode \n",
    "    \n",
    "    iterator.init_epoch()  #TODO create_batches \n",
    "\n",
    "    with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "            \n",
    "            claims_batch = [torch.LongTensor(example['claim']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "            evidences_batch = [torch.LongTensor(example['evidence']) for example in batch]     #list of tensors of tags id for each sentence in a batch \n",
    "\n",
    "            claims_lengths = torch.Tensor([len(example['claim']) for example in batch])         #lenght of each claim sentence before padding \n",
    "            evidence_lengths = torch.Tensor([len(example['evidence']) for example in batch])         #lenght of each evidence sentence before padding \n",
    "\n",
    "            target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "            target_ids = torch.LongTensor([example['claim_id'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "            #move tensors to gpu if possible \n",
    "            claims_lengths = claims_lengths.to(device)\n",
    "            evidence_lengths = evidence_lengths.to(device)\n",
    "            target_labels = target_labels.to(device)    \n",
    "\n",
    "            \n",
    "            predictions = model(claims_batch,claims_lengths,evidences_batch,evidence_lengths)   #generate predictions \n",
    "\n",
    "            loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "            \n",
    "            pred = (predictions > 0.0 ).int().cpu()         #get class label \n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred = torch.cat((all_pred,pred))          \n",
    "            all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "            all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "            batch_loss += loss.item()   #accumulate batch loss \n",
    "            \n",
    "\n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    maj_acc, maj_f1 = acc_and_f1_majority(all_targ,all_pred,all_ids)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)   #mean loss \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('eval epoch time: %s',end-start)\n",
    "\n",
    "    return loss, acc, f1, maj_acc, maj_f1, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the train and eval loop we can combine them. Those two phases will be alternated for each epoch in order to see the progresses made by our model.\\\n",
    "Here we also instantiate the optimizer, the loss criterion and the model itself with the parameters that we specify.\\\n",
    "The `train_and_eval` function takes as parameter the specific model that will be trained and evaluated in turn over the entire dataset, the mini-batches defined previously and the hyperparameters.\\\n",
    "A dictionary `model_metrics` containing the results for each of these architectures is then returned. For each architecture it contains:\n",
    "- `model_name`: the name of the model evaluated. \n",
    "- `train_loss`, `train_acc`, `train_f1`: standard metrics computed at each epoch on the training set.\n",
    "- `train_maj_acc`, `train_maj_f1`: metrics computed using the majority voting technique on the training set.\n",
    "- `val_loss`, `val_acc`, `val_f1`: standard metrics computed at each epoch on the validation set.\n",
    "- `val_maj_acc`, `val_maj_f1`: metrics computed using the majority voting technique on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model: Custom_model, dataloaders: tuple[BucketIterator,...], param : dict(), device):\n",
    "    \"\"\"\n",
    "        Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "    \"\"\"\n",
    "    best_f1, best_epoch = -1, -1   #init best f1 score \n",
    "    model_name = model.get_name()\n",
    "\n",
    "    log.debug('Train and Eval for model: %s \\n',model_name)\n",
    "\n",
    "    model_metrics = {\n",
    "        \"model_name\": model_name,\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"train_maj_acc\": [],\n",
    "        \"train_maj_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "        \"val_maj_acc\": [],\n",
    "        \"val_maj_f1\": [],\n",
    "    }\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=param['lr'],  weight_decay=param['weight_decay'])   #L2 regularization \n",
    "\n",
    "    train_dataloader, eval_dataloader = dataloaders   #unpack dataloaders \n",
    "\n",
    "    for epoch in range(param['n_epochs']): #epoch loop\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        train_metrics = train_loop(model, train_dataloader, optimizer, criterion, device) #train\n",
    "        eval_metrics = eval_loop(model, eval_dataloader, criterion, device) #eval\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        tot_epoch_time = end_time-start_time          \n",
    "\n",
    "        train_epoch_loss, train_epoch_acc, train_epoch_f1, train_epoch_maj_acc, train_epoch_maj_f1, train_predictions = train_metrics\n",
    "        eval_epoch_loss, eval_epoch_acc, eval_epoch_f1, eval_epoch_maj_acc, eval_epoch_maj_f1, eval_predictions = eval_metrics\n",
    "\n",
    "        if eval_epoch_f1 >= best_f1:\n",
    "            best_f1 = eval_epoch_f1\n",
    "            best_epoch = epoch+1\n",
    "            if not os.path.exists('models'):        \n",
    "                os.makedirs('models')\n",
    "            torch.save(model.state_dict(),f'models/{model_name}.pt')\n",
    "\n",
    "\n",
    "        #log Train and Validation metrics\n",
    "        model_metrics['train_loss'].append(train_epoch_loss)\n",
    "        model_metrics['train_acc'].append(train_epoch_acc)\n",
    "        model_metrics['train_f1'].append(train_epoch_f1)\n",
    "        model_metrics['train_maj_acc'].append(train_epoch_maj_acc)\n",
    "        model_metrics['train_maj_f1'].append(train_epoch_maj_f1)\n",
    "        model_metrics['val_loss'].append(eval_epoch_loss)\n",
    "        model_metrics['val_acc'].append(eval_epoch_acc)\n",
    "        model_metrics['val_f1'].append(eval_epoch_f1)\n",
    "        model_metrics['val_maj_acc'].append(eval_epoch_maj_acc)\n",
    "        model_metrics['val_maj_f1'].append(eval_epoch_maj_f1)\n",
    "       \n",
    "        \n",
    "        log.debug('Elapsed time for epoch %s : %s \\n',epoch+1,tot_epoch_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "        print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f} | Train Maj Acc: {train_epoch_maj_acc*100:.2f}%  | Train Maj F1: {train_epoch_maj_f1:.2f}')\n",
    "        print(f'\\t Val. Loss: {eval_epoch_loss:.3f} | Val. Acc: {eval_epoch_acc*100:.2f}% | Val. F1: {eval_epoch_f1:.2f}  | Val Maj Acc: {eval_epoch_maj_acc*100:.2f}%  | Val Maj F1: {eval_epoch_maj_f1:.2f}')\n",
    "    \n",
    "    log.debug('Best Eval F1: %s, obtained at epoch: %s \\n\\n',best_f1,best_epoch)\n",
    "\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models we have to define some useful parameters and hyperparameters to pass to the train and validation loops. According to our experimental results obtained after a tuning phase, these are the ones that we are using to train each architecture:\n",
    "- `BATCH_SIZE = 128`: we have experimented with the sizes a little bit, but we don't want batches to be neither too small, to avoid noise in the gradients, nor too big, to speed up the process. Since we didn't see big differences for a batch size range between 64 and 256, we decided to set `BATCH_SIZE = 128`.\n",
    "- `LR = 1e-3`: we obtained the best results using `Adam`, which automatically adapts the learning rate at each epoch. Since it allows you to choose the size you want to start with, we got the best results by using `1e-3`.\n",
    "- `N_EPOCHS = 15`: with `DROPOUT = True`, we see that the models achieve the best performance nearly after 15 epochs, remaining stable after that.\n",
    "- `WEIGHT_DECAY = 1e-5`: to avoid overfitting we introduced an L2 regularization.\n",
    "- `FREEZE = False`: if set to `False`, the parameters of the embedding layer become trainable, adding another trainable parameter which resulted useful to obtain the best outcomes.\n",
    "- `DROPOUT = True`: technique used to avoid overfitting which resulted useful in particular for the `mlp` and `bag_of_vectors` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "PAD_IDX = 0                     # pad index\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 128                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "N_EPOCHS = 15                   # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "\n",
    "#model parameters\n",
    "FREEZE = False                  # wheter to make the embedding layer trainable or not\n",
    "RNN = 'lstm'                    # either gru or lstm\n",
    "DROPOUT = False                 # wheter to use dropout layer or not  \n",
    "\n",
    "\n",
    "#to counteract class imbalance \n",
    "(supports, refutes) = df.loc[df['split'] == 'train']['idx_label'].value_counts()    #number of supports and refutes in the train dataset \n",
    "weight_positive_class = torch.Tensor([refutes/supports]).to(DEVICE)  #weight to give to positive class \n",
    "\n",
    "max_tokens = max(df.claim.apply(len).max(),df.evidence.apply(len).max())  #max number of tokens in a sentence in the entire dataset \n",
    "\n",
    "\n",
    "#train pipeline parameters dictionary \n",
    "train_param = {\n",
    "    'lr': LR,\n",
    "    'n_epochs': N_EPOCHS,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'weight_positive_class': weight_positive_class\n",
    "    }\n",
    "\n",
    "#model parameters dictionary\n",
    "model_param = {\n",
    "    'pad_idx' : PAD_IDX,\n",
    "    'max_tokens' : max_tokens,\n",
    "    'freeze_embedding' : FREEZE,   \n",
    "    'rnn' : RNN,\n",
    "    'dropout' : DROPOUT\n",
    "}\n",
    "\n",
    "#create dataloaders \n",
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Train the Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our models. In particular we are going to train 4 different models, with different sentence embedding strategy while keeping the same claim-evidence input merging strategy. \n",
    "We decided to chose as input-merging baseline the `concat` strategy which is the one that operate less manipulation on the two inputs of the classifier (claim and evidence) by just concatenating them.\n",
    "\n",
    "In this way we are evaluating all the possible sentence embedding strategies in order to pick the best performing one that will be then tested also with the other input merging options.\n",
    "In the end the very best model architecture will be also trained with the additional feature of `cosine similarity` to the classification input. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory \n",
    "import gc\n",
    "def clean_gpu_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_models_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) MLP \n",
    "Here we train the model with the `mlp` strategy that encodes the sentences via a Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture1 = Architecture('mlp','concat',False)                    \n",
    "model1 = Custom_model(embedding_matrix, model_param, architecture1, DEVICE)\n",
    "\n",
    "model1_metrics = train_and_eval(model1, (train_dataloader,val_dataloader), train_param, DEVICE)\n",
    "\n",
    "all_models_metrics[model1_metrics['model_name']] = model1_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) RNNLastState \n",
    "Here we train the model with the `RNNLastState` strategy. It encodes token sequences via a RNN and takes the last state as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "\n",
    "architecture2 = Architecture('rnn_last','concat',False)                    \n",
    "model2 = Custom_model(embedding_matrix, model_param, architecture2, DEVICE)\n",
    "\n",
    "model2_metrics = train_and_eval(model2, (train_dataloader,val_dataloader), train_param,DEVICE)\n",
    "\n",
    "all_models_metrics[model2_metrics['model_name']] = model2_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) RNNAvg \n",
    "Here we train the model with the `RNNAvg` strategy that consists in encoding the sentences via a RNN and then averages all the output states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "\n",
    "architecture3 = Architecture('rnn_avg','concat',False)                    \n",
    "model3 = Custom_model(embedding_matrix, model_param, architecture3, DEVICE)\n",
    "\n",
    "model3_metrics = train_and_eval(model3, (train_dataloader,val_dataloader), train_param,DEVICE)\n",
    "\n",
    "all_models_metrics[model3_metrics['model_name']] = model3_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) BagOfVectors\n",
    "Here we train the model with the `BagOfVectors` strategy. The sentence embedding is computed as the mean of its token embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "\n",
    "architecture4 = Architecture('bag_of_vectors','concat',False)                    \n",
    "model4 = Custom_model(embedding_matrix, model_param, architecture4, DEVICE)\n",
    "\n",
    "model4_metrics = train_and_eval(model4, (train_dataloader,val_dataloader), train_param,DEVICE)\n",
    "\n",
    "all_models_metrics[model4_metrics['model_name']] = model4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on to try different input-merging strategies, let's first plot the results of these first four runs of train and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = ['train_loss','val_loss', 'train_acc','val_acc', 'train_f1', 'val_f1', 'train_maj_acc', 'val_maj_acc', 'train_maj_f1', 'val_maj_f1']\n",
    "metrics = ['Loss', 'Accuracy', 'F1-Score', 'Majority Accuracy', 'Majority F1-Score']\n",
    "rows = [name for name in all_models_metrics.keys()]\n",
    "colors = ['lightsalmon', 'red', 'lightblue', 'blue', 'lightgreen', 'green', 'mediumorchid', 'darkviolet', 'gold', 'goldenrod']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(rows), ncols=len(metrics), figsize=(20, 20))\n",
    "\n",
    "for ax, col in zip(axes[0], metrics):\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, size='large')\n",
    "\n",
    "keys = list(all_models_metrics.keys())\n",
    "\n",
    "for plt_row in range(len(rows)):\n",
    "    num_metric = 0\n",
    "    for plt_col in range(len(metrics)):\n",
    "        axes[plt_row,plt_col].plot(all_models_metrics[keys[plt_row]][cols[num_metric]], color= colors[num_metric], label = 'Train') #plot train metrics\n",
    "        axes[plt_row,plt_col].plot(all_models_metrics[keys[plt_row]][cols[num_metric+1]], color= colors[num_metric+1], label = 'Val') #plot validation metrics\n",
    "        axes[plt_row,plt_col].legend(loc=\"best\")\n",
    "        num_metric += 2\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above graph, the RNN architectures perform slightly better than the other ones. In particular, `rnn_avg` seems to be the best performing sentence embedding, reaching an F1-score of 0.77 on the validation set in relatively few epochs. On the other hand, `rnn_last` usually performs a bit worse, probably because in this case only the last result of the network is considered and there is not a general evaluation based on all the outputs. Despite this, even `mlp` and `bag_of_vectors`, which are much simpler sentence embedding techniques than the previous ones, achieve acceptable levels of accuracy, usually reaching an F1-score around 0.72 and 0.73. Finally, the two techniques of regularization, L2 and Dropout, were fundamental to avoid overfitting, which is however present after a certain number of epochs but in a very minor way.\\\n",
    "\\\n",
    "At this point we can select the best performing sentence embedding strategy (`rnn_avg`) and train that architecture with all possible input merging strategies. Of course we won't need to test again the `concat` strategy with which we did the previous runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Sum\n",
    "Here we train the model with the `Sum` input-merging strategy. The input that will be passed to the classifier is, in this case, the sum of evidence and claim sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "\n",
    "architecture5 = Architecture('rnn_avg','sum',False)                    \n",
    "model5 = Custom_model(embedding_matrix, model_param, architecture5, DEVICE)\n",
    "\n",
    "model5_metrics = train_and_eval(model5, (train_dataloader,val_dataloader), train_param, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Mean\n",
    "Here we train the model with the `Mean` input-merging strategy. The final classification input is here defined as the mean of evidence and claim sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "\n",
    "architecture6 = Architecture('rnn_avg','mean',False)                    \n",
    "model6 = Custom_model(embedding_matrix, model_param, architecture6, DEVICE)\n",
    "\n",
    "model6_metrics = train_and_eval(model6, (train_dataloader,val_dataloader), train_param, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTI SUI RISULTATI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can add an additional feature to the classification input of our best neural architecture. This additional value is the cosine similarity between the two sentence embeddings (claim and evidence) and it is concatenated to the input of the classifier.\n",
    "\n",
    "In the end we will now train and evaluate a model whose architecture has `rnn_avg` as sentence embedding strategy, `dipende quale è meglio` as claim-evidence merging strategy, and the additional feature of `cosine similarity` added to the classification input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "\n",
    "architecture7 = Architecture('rnn_avg','concat',True)                    \n",
    "model7 = Custom_model(embedding_matrix, model_param, architecture7, DEVICE)\n",
    "\n",
    "model7_metrics = train_and_eval(model7, (train_dataloader,val_dataloader), train_param, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTI SUI RISULTATI , ANALYSYS , SUMMARY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
